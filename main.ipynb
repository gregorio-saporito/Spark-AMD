{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNuiv3HySz0HElk1euN7qL6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregorio-saporito/Spark-AMD/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgrgJf1btZqq"
      },
      "source": [
        "# Finding similar items: StackSample\n",
        "Gregorio Luigi Saporito - DSE (2020-2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06XBf9b2orbU"
      },
      "source": [
        "### Upload to session storage the Kaggle API token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSKqPMQcETJ9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "5a727acc-89df-4d60-9b43-8adca2c25169"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c56c9a95-699f-4c38-94bc-f58d610143ca\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c56c9a95-699f-4c38-94bc-f58d610143ca\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xZHccNro4qU"
      },
      "source": [
        "### Download the dataset through the Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKapvesjNkIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd90d56c-e9be-45e0-b131-1e291f42d479"
      },
      "source": [
        "# access permissions with the API token\n",
        "!chmod 600 /content/kaggle.json\n",
        "!kaggle datasets download -d stackoverflow/stacksample\n",
        "!unzip \\*.zip && rm *.zip\n",
        "# remove datasets which are not needed\n",
        "!rm Answers.csv\n",
        "!rm Tags.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading stacksample.zip to /content\n",
            "100% 1.11G/1.11G [00:14<00:00, 70.1MB/s]\n",
            "100% 1.11G/1.11G [00:14<00:00, 81.3MB/s]\n",
            "Archive:  stacksample.zip\n",
            "  inflating: Answers.csv             \n",
            "  inflating: Questions.csv           \n",
            "  inflating: Tags.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-et_dbzvp7GA"
      },
      "source": [
        "### Spark environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKZvjqwGvvkH"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz \n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!rm /content/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-3.1.1-bin-hadoop2.7\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "import pyspark\n",
        "type(spark)\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIKSjeUjSMKV"
      },
      "source": [
        "### Load Dataset\n",
        "Spark reads files line by line for performance reasons and CSVs with newline characters cause problems for the parser. In this case the Body column of the file \"Questions.csv\" has characters like `\"\\n\"` and `\"\\r\"` which compromise the correct loading of the dataset. When the parser finds a newline character in a string it loads the remaining part in a new line incorrectly splitting the string and loading the remaning part of the string in a new line of the DataFrame which often corresponds to a wrong column (most of the remaining columns are then filled with null values). To solve this problem the escape character `\"\\\"\"` is manually specified as an option when loading the .csv file with Spark, resulting in the correct loading of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7361417UcG8N",
        "outputId": "74e9ddd6-2029-479b-faa0-81675181cadd"
      },
      "source": [
        "df = spark.read\\\n",
        "  .option(\"multiLine\", \"true\")\\\n",
        "  .option(\"header\", \"true\")\\\n",
        "  .option(\"escape\", \"\\\"\")\\\n",
        "  .csv(\"Questions.csv\")\\\n",
        "  .select(\"Id\",\"Body\")\n",
        "\n",
        "df.show(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| Id|                Body|\n",
            "+---+--------------------+\n",
            "| 80|<p>I've written a...|\n",
            "| 90|<p>Are there any ...|\n",
            "|120|<p>Has anyone got...|\n",
            "|180|<p>This is someth...|\n",
            "|260|<p>I have a littl...|\n",
            "|330|<p>I am working o...|\n",
            "|470|<p>I've been writ...|\n",
            "|580|<p>I wonder how y...|\n",
            "|650|<p>I would like t...|\n",
            "|810|<p>I'm trying to ...|\n",
            "+---+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlfeAPo-gMgn",
        "outputId": "4ffca815-8e16-4192-9a0e-18f89bbd5b12"
      },
      "source": [
        "# how many observations\n",
        "df.count()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1264216"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hrL4t0jdpPl"
      },
      "source": [
        "### Dataset Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPsncJC2duQD",
        "outputId": "737ce8c1-a681-4bb2-e93f-b955007b3ad7"
      },
      "source": [
        "# check for missing values\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+\n",
            "| Id|Body|\n",
            "+---+----+\n",
            "|  0|   0|\n",
            "+---+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQhEj-btxT6c"
      },
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
        "\n",
        "def clean_html(x):\n",
        "  x = regexp_replace(x, '<.*?>', '')\n",
        "  x = regexp_replace(x, '\\n|\\r', '')\n",
        "  return x\n",
        "\n",
        "df = df.select(\"Id\", clean_html(col(\"Body\")).alias(\"Body\"))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzzQpg-7NXKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05cbe82e-504b-4bac-d5e3-c381fe87f9dc"
      },
      "source": [
        "df.show(10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| Id|                Body|\n",
            "+---+--------------------+\n",
            "| 80|I've written a da...|\n",
            "| 90|Are there any rea...|\n",
            "|120|Has anyone got ex...|\n",
            "|180|This is something...|\n",
            "|260|I have a little g...|\n",
            "|330|I am working on a...|\n",
            "|470|I've been writing...|\n",
            "|580|I wonder how you ...|\n",
            "|650|I would like the ...|\n",
            "|810|I'm trying to mai...|\n",
            "+---+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLvKywjeVK5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680ab929-e5e9-4e0b-f0c2-161ebb4126d2"
      },
      "source": [
        "# extracting tokens from text\n",
        "from pyspark.ml.feature import RegexTokenizer\n",
        "\n",
        "regexTokenizer = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'Body', outputCol = 'tokens')\n",
        "tokenised = regexTokenizer.transform(df)\n",
        "tokenised.show(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+--------------------+\n",
            "| Id|                Body|              tokens|\n",
            "+---+--------------------+--------------------+\n",
            "| 80|I've written a da...|[i, ve, written, ...|\n",
            "| 90|Are there any rea...|[are, there, any,...|\n",
            "|120|Has anyone got ex...|[has, anyone, got...|\n",
            "+---+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf6ofShrVvex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4a8dc2-ba65-4a30-823a-472fd280283a"
      },
      "source": [
        "# stopwords removal\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "swr = StopWordsRemover(inputCol = 'tokens', outputCol = 'sw_removed')\n",
        "Body_swr = swr.transform(tokenised)\n",
        "Body_swr.show(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+--------------------+--------------------+\n",
            "| Id|                Body|              tokens|          sw_removed|\n",
            "+---+--------------------+--------------------+--------------------+\n",
            "| 80|I've written a da...|[i, ve, written, ...|[ve, written, dat...|\n",
            "| 90|Are there any rea...|[are, there, any,...|[really, good, tu...|\n",
            "|120|Has anyone got ex...|[has, anyone, got...|[anyone, got, exp...|\n",
            "+---+--------------------+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoHta0i2HOSR"
      },
      "source": [
        "### Jaccard Distance Approach with LSH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUfRLpdIPZdb"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import HashingTF, MinHashLSH\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import monotonically_increasing_id \n",
        "\n",
        "jd_df = Body_swr\\\n",
        "  .withColumn(\"nodup\", f.array_distinct(\"sw_removed\"))\\\n",
        "  .select(\"Id\",\"nodup\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y5K_rtgQf4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b0705a-e337-4557-bdab-d939e6664134"
      },
      "source": [
        "jd_df.show(3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+\n",
            "| Id|               nodup|\n",
            "+---+--------------------+\n",
            "| 80|[ve, written, dat...|\n",
            "| 90|[really, good, tu...|\n",
            "|120|[anyone, got, exp...|\n",
            "+---+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxq90crpYPgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a649d6ee-87ce-4481-bc35-07655ae9093f"
      },
      "source": [
        "jd_df.printSchema()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- nodup: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4aT-gj7i_p0"
      },
      "source": [
        "from pyspark.sql.functions import col, size\n",
        "# filer out empty arrays of strings and use a smaller chunk of the dataset\n",
        "jd_df_clean = jd_df.where((size(col(\"nodup\")) >= 1)).limit(20000)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TRELBRZmb0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fd82ee-7bfd-43ef-9505-4d3af0e82d79"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "model = Pipeline(stages=[\n",
        "        HashingTF(inputCol=\"nodup\", outputCol=\"vectors\"),\n",
        "        MinHashLSH(inputCol=\"vectors\", outputCol=\"lsh\", seed=123)\n",
        "    ]).fit(jd_df_clean)\n",
        "\n",
        "db_hashed = model.transform(jd_df_clean)\n",
        "\n",
        "print(\"Approximately joining on distance smaller than 0.5:\")\n",
        "db_matches = model.stages[-1].approxSimilarityJoin(db_hashed, db_hashed, 0.5)\n",
        "\n",
        "#show all matches (including duplicates)\n",
        "db_matches.select(f.col('datasetA.Id').alias('id_A'),\n",
        "                 f.col('datasetB.Id').alias('id_B'),\n",
        "                 f.col('distCol')).show()\n",
        "\n",
        "end = time.time()\n",
        "print(round(end-start,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Approximately joining on distance smaller than 0.5:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa37_1-PeLqY"
      },
      "source": [
        "#show non-duplicate matches\n",
        "db_matches.select(f.col('datasetA.id').alias('id_A'),\n",
        "                 f.col('datasetB.id').alias('id_B'),\n",
        "                 f.col('distCol')).filter('id_A < id_B').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No9Y4qWmfMPH"
      },
      "source": [
        "# example 1\n",
        "df.where((col(\"id\") == 612820) | (col(\"id\") == 634630)).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaQsgam6u0hq"
      },
      "source": [
        "# example 2\n",
        "df.where((col(\"id\") == 1041520) | (col(\"id\") == 1042370)).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-UEFaSAAede"
      },
      "source": [
        "### Some performance considerations\n",
        "A series of tests on the computational performance of Spark in Google Colab were performed to track the computational time required to run the LSH algorithm as a function of the dataset size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTwf7ClEDD_r"
      },
      "source": [
        "import pandas as pd\n",
        "d = {'size': [100, 200, 1000, 5000, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000],\n",
        "     'time': [1.31, 1.46, 3.33, 12.42, 33.32, 111.91, 229.47, 403.05, 642.67, 891.45, 868.92, 857.02]}\n",
        "df_time = pd.DataFrame(data=d)\n",
        "df_time.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOyPThEOKr2Z"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x = df_time['size']\n",
        "y = df_time['time']\n",
        "plt.plot(x, y)\n",
        "plt.scatter(x, y)\n",
        "\n",
        "z = np.polyfit(x, y, 1)\n",
        "p = np.poly1d(z)\n",
        "plt.plot(x,p(x),\"g--\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRkdsCxJNqyR"
      },
      "source": [
        "Assuming a linear trend, how long would it take to process the entire StackSample dataset (around 1.2 million observations)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OkBiVUZN0O2"
      },
      "source": [
        "import datetime\n",
        "predict = np.poly1d(z)\n",
        "size_needed = 1200000\n",
        "prediction = predict(size_needed)\n",
        "conversion = datetime.timedelta(seconds=prediction)\n",
        "str(conversion)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}